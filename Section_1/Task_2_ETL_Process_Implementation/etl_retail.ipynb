{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99218ab3",
   "metadata": {},
   "source": [
    "# *ETL Process for Retail Data Warehouse*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1db178",
   "metadata": {},
   "source": [
    "#### *Load and preprocess the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e452857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from glob import glob\n",
    "import hashlib\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ca9aa",
   "metadata": {},
   "source": [
    "# *Data Generation: Dimensions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748c929",
   "metadata": {},
   "source": [
    "#### *Generate synthetic Customer Dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8917814",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "Faker.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffea8cd",
   "metadata": {},
   "source": [
    "*Description:*\n",
    "\n",
    "*This chunk focuses on loading the raw dataset from a CSV file into a pandas DataFrame, ensuring that special characters in the data are correctly handled by specifying the appropriate encoding. The 'InvoiceDate' column is explicitly converted into a datetime object, which enables more efficient and accurate manipulation of date and time data. To make the dataset appear current for analysis purposes, all invoice dates are shifted forward by 14 years, adjusting the original 2010-2011 timestamps to approximately 2024-2025. This simulated recency of the data can be important for testing or reporting. Finally, the new date range is printed as a sanity check to confirm that the shift was applied correctly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e508ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range after shifting: 2024-12-01 08:26:00 to 2025-12-09 12:50:00\n"
     ]
    }
   ],
   "source": [
    "# Load dataset CSV into pandas DataFrame.\n",
    "# Encoding ISO-8859-1 is used to handle special characters.\n",
    "df = pd.read_csv('../Data/Online_Retail.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Convert 'InvoiceDate' column to datetime type for easier date/time operations.\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Shift invoice dates forward by 14 years to simulate current data (2024-2025).\n",
    "df['InvoiceDate'] = df['InvoiceDate'] + pd.DateOffset(years=14)\n",
    "\n",
    "# Verify date range after shifting.\n",
    "print(\"Date range after shifting:\", df['InvoiceDate'].min(), \"to\", df['InvoiceDate'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb481e",
   "metadata": {},
   "source": [
    "### *Create Time Dimension Table (TimeDim)*\n",
    "*Description:*\n",
    "\n",
    "*In this step, a Time Dimension table is constructed, which is fundamental in data warehousing and analytical processing for providing rich temporal context to sales data. The process starts by extracting all unique dates from the transactional data and normalizing them to remove time components, ensuring that each date appears only once. A unique `TimeID` is generated for each date using the YYYYMMDD format, facilitating efficient joins with fact tables. Additional columns are created to break down each date into components such as day, month, quarter, year, and week number — all valuable for time-based grouping and trend analysis. This denormalized structure simplifies querying and reporting over time.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155e0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Time Dimension (TimeDim) table\n",
    "# Create empty DataFrame for Time Dimension\n",
    "time_dim = pd.DataFrame()\n",
    "\n",
    "# Extract unique dates from the 'InvoiceDate' column (date only, no time)\n",
    "time_dim['FullDate'] = pd.to_datetime(df['InvoiceDate'].dt.date.unique())\n",
    "\n",
    "# Generate a unique TimeID for each date in YYYYMMDD integer format\n",
    "time_dim['TimeID'] = time_dim['FullDate'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# Extract useful date attributes for analysis\n",
    "time_dim['Day'] = time_dim['FullDate'].dt.day\n",
    "time_dim['Month'] = time_dim['FullDate'].dt.month\n",
    "time_dim['Quarter'] = time_dim['FullDate'].dt.quarter\n",
    "time_dim['Year'] = time_dim['FullDate'].dt.year\n",
    "time_dim['WeekOfYear'] = time_dim['FullDate'].dt.isocalendar().week\n",
    "\n",
    "# Reorder columns for clarity\n",
    "time_dim = time_dim[['TimeID', 'FullDate', 'Day', 'Month', 'Quarter', 'Year', 'WeekOfYear']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a56ff",
   "metadata": {},
   "source": [
    "### *Create Customer Dimension Table (CustomerDim)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e72f7b",
   "metadata": {},
   "source": [
    "*Description:*\n",
    "\n",
    "*This chunk builds the Customer Dimension table, which profiles unique customers using a combination of actual and synthetic data. The real `CustomerID` and country information are directly extracted to maintain referential integrity. Since personal identifying information like names and cities are not available or desirable to use, synthetic values are generated to enrich the dataset while preserving privacy. Customer names are created by hashing the `CustomerID` to produce consistent yet anonymous identifiers. Cities are generated using the Faker library with locale settings based on the customer’s country, adding realistic geographic diversity. Additionally, plausible gender and age values are randomly assigned within reasonable bounds to simulate demographic attributes. Finally, the earliest invoice date is used as a proxy for the customer’s registration date, providing a temporal reference for customer activity.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2808f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CustomerID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CustomerName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "City",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CustomerSince",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "832842ed-305a-4a62-95b3-5f952971b73b",
       "rows": [
        [
         "0",
         "17850.0",
         "United Kingdom",
         "54cde5dbb6",
         "North Henrybury",
         "Other",
         "71",
         "2024-12-01 08:26:00"
        ],
        [
         "9",
         "13047.0",
         "United Kingdom",
         "86314fa849",
         "East Timothy",
         "Male",
         "69",
         "2024-12-01 08:26:00"
        ],
        [
         "26",
         "12583.0",
         "France",
         "dcff63cd99",
         "New Roberttown",
         "Male",
         "74",
         "2024-12-01 08:26:00"
        ],
        [
         "46",
         "13748.0",
         "United Kingdom",
         "590354e49f",
         "East Donaldhaven",
         "Other",
         "49",
         "2024-12-01 08:26:00"
        ],
        [
         "65",
         "15100.0",
         "United Kingdom",
         "58ec7997a6",
         "New Joeside",
         "Female",
         "52",
         "2024-12-01 08:26:00"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "      <th>CustomerName</th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>CustomerSince</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>54cde5dbb6</td>\n",
       "      <td>North Henrybury</td>\n",
       "      <td>Other</td>\n",
       "      <td>71</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13047.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>86314fa849</td>\n",
       "      <td>East Timothy</td>\n",
       "      <td>Male</td>\n",
       "      <td>69</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12583.0</td>\n",
       "      <td>France</td>\n",
       "      <td>dcff63cd99</td>\n",
       "      <td>New Roberttown</td>\n",
       "      <td>Male</td>\n",
       "      <td>74</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>13748.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>590354e49f</td>\n",
       "      <td>East Donaldhaven</td>\n",
       "      <td>Other</td>\n",
       "      <td>49</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>15100.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>58ec7997a6</td>\n",
       "      <td>New Joeside</td>\n",
       "      <td>Female</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CustomerID         Country CustomerName              City  Gender  Age  \\\n",
       "0      17850.0  United Kingdom   54cde5dbb6   North Henrybury   Other   71   \n",
       "9      13047.0  United Kingdom   86314fa849      East Timothy    Male   69   \n",
       "26     12583.0          France   dcff63cd99    New Roberttown    Male   74   \n",
       "46     13748.0  United Kingdom   590354e49f  East Donaldhaven   Other   49   \n",
       "65     15100.0  United Kingdom   58ec7997a6       New Joeside  Female   52   \n",
       "\n",
       "         CustomerSince  \n",
       "0  2024-12-01 08:26:00  \n",
       "9  2024-12-01 08:26:00  \n",
       "26 2024-12-01 08:26:00  \n",
       "46 2024-12-01 08:26:00  \n",
       "65 2024-12-01 08:26:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "def hash_customer_name(cust_id):\n",
    "    # Generate a synthetic name by hashing the CustomerID\n",
    "    return hashlib.sha256(str(cust_id).encode()).hexdigest()[:10]\n",
    "\n",
    "def generate_city_based_on_country(country):\n",
    "    # Use Faker locale based on country for city name if possible, else default locale\n",
    "    # Here we simplify: if country is UK use en_GB, else en_US or default\n",
    "    if country == 'United Kingdom':\n",
    "        fake_local = Faker('en_GB')\n",
    "    else:\n",
    "        fake_local = Faker()\n",
    "    return fake_local.city()\n",
    "\n",
    "# Extract unique customers with country\n",
    "customer_dim = df[['CustomerID', 'Country']].drop_duplicates().copy()\n",
    "\n",
    "# Create synthetic CustomerName by hashing CustomerID\n",
    "customer_dim['CustomerName'] = customer_dim['CustomerID'].apply(hash_customer_name)\n",
    "\n",
    "# Generate synthetic City based on Country\n",
    "customer_dim['City'] = customer_dim['Country'].apply(generate_city_based_on_country)\n",
    "\n",
    "# Generate reasonable synthetic Gender and Age\n",
    "gender_choices = ['Male', 'Female', 'Other']\n",
    "customer_dim['Gender'] = [random.choice(gender_choices) for _ in range(len(customer_dim))]\n",
    "customer_dim['Age'] = [random.randint(18, 75) for _ in range(len(customer_dim))]\n",
    "\n",
    "# Set CustomerSince as earliest InvoiceDate in the dataset\n",
    "customer_dim['CustomerSince'] = df['InvoiceDate'].min()\n",
    "\n",
    "# Drop Email column (not required)\n",
    "# No Email column added here\n",
    "\n",
    "customer_dim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c2de9",
   "metadata": {},
   "source": [
    "### *Create Store Dimension Table (StoreDim)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c637d7",
   "metadata": {},
   "source": [
    "\n",
    "*Description:*\n",
    "\n",
    "*This step constructs the Store Dimension table, which represents the stores or sales locations for transactions. Since the dataset primarily references countries rather than specific store locations, each unique country is treated as a distinct store. Unique numeric `StoreID`s are assigned for efficient foreign key references. Store names are generated to include the country name for clarity and uniqueness. The sales channel is hardcoded as \"Online,\" reflecting the dataset's nature as online retail transactions. To provide richer location information, synthetic cities are generated for each store based on the country, using locale-specific Faker instances to maintain geographic plausibility. This approach allows analysis at the store level while enhancing location details without requiring real-world addresses.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c0afb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "StoreID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "StoreName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Channel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "City",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "eb8ca5aa-783d-46d8-bd34-a8338e69738e",
       "rows": [
        [
         "0",
         "United Kingdom",
         "1",
         "Online Store - United Kingdom",
         "Online",
         "Kimberleychester"
        ],
        [
         "1",
         "France",
         "2",
         "Online Store - France",
         "Online",
         "Higginston"
        ],
        [
         "2",
         "Australia",
         "3",
         "Online Store - Australia",
         "Online",
         "Deborahmouth"
        ],
        [
         "3",
         "Netherlands",
         "4",
         "Online Store - Netherlands",
         "Online",
         "Berryhaven"
        ],
        [
         "4",
         "Germany",
         "5",
         "Online Store - Germany",
         "Online",
         "South Lisachester"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>StoreID</th>\n",
       "      <th>StoreName</th>\n",
       "      <th>Channel</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1</td>\n",
       "      <td>Online Store - United Kingdom</td>\n",
       "      <td>Online</td>\n",
       "      <td>Kimberleychester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>2</td>\n",
       "      <td>Online Store - France</td>\n",
       "      <td>Online</td>\n",
       "      <td>Higginston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>3</td>\n",
       "      <td>Online Store - Australia</td>\n",
       "      <td>Online</td>\n",
       "      <td>Deborahmouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>4</td>\n",
       "      <td>Online Store - Netherlands</td>\n",
       "      <td>Online</td>\n",
       "      <td>Berryhaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>5</td>\n",
       "      <td>Online Store - Germany</td>\n",
       "      <td>Online</td>\n",
       "      <td>South Lisachester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country  StoreID                      StoreName Channel  \\\n",
       "0  United Kingdom        1  Online Store - United Kingdom  Online   \n",
       "1          France        2          Online Store - France  Online   \n",
       "2       Australia        3       Online Store - Australia  Online   \n",
       "3     Netherlands        4     Online Store - Netherlands  Online   \n",
       "4         Germany        5         Online Store - Germany  Online   \n",
       "\n",
       "                City  \n",
       "0   Kimberleychester  \n",
       "1         Higginston  \n",
       "2       Deborahmouth  \n",
       "3         Berryhaven  \n",
       "4  South Lisachester  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique countries as stores\n",
    "store_dim = df[['Country']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Assign unique StoreID starting from 1\n",
    "store_dim['StoreID'] = store_dim.index + 1\n",
    "\n",
    "# Assign StoreName with country suffix for uniqueness\n",
    "store_dim['StoreName'] = store_dim['Country'].apply(lambda x: f\"Online Store - {x}\")\n",
    "\n",
    "# Assign Channel as 'Online' (dataset is online retail)\n",
    "store_dim['Channel'] = 'Online'\n",
    "\n",
    "# Generate synthetic City based on country using Faker locales\n",
    "def generate_city(country):\n",
    "    if country == 'United Kingdom':\n",
    "        fake_local = Faker('en_GB')\n",
    "    else:\n",
    "        fake_local = Faker()\n",
    "    return fake_local.city()\n",
    "\n",
    "store_dim['City'] = store_dim['Country'].apply(generate_city)\n",
    "\n",
    "store_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aade59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ProductID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ProductName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "UnitCost",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Brand",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "53f6f1c8-e783-465f-833d-fc9e434932d8",
       "rows": [
        [
         "0",
         "85123A",
         "WHITE HANGING HEART T-LIGHT HOLDER",
         "2.55",
         "Miscellaneous",
         "Carpenter, Burton and Oneal"
        ],
        [
         "1",
         "71053",
         "WHITE METAL LANTERN",
         "3.39",
         "Miscellaneous",
         "Francis-Mann"
        ],
        [
         "2",
         "84406B",
         "CREAM CUPID HEARTS COAT HANGER",
         "2.75",
         "Miscellaneous",
         "Lara-Baker"
        ],
        [
         "3",
         "84029G",
         "KNITTED UNION FLAG HOT WATER BOTTLE",
         "3.39",
         "Miscellaneous",
         "Diaz-Schaefer"
        ],
        [
         "4",
         "84029E",
         "RED WOOLLY HOTTIE WHITE HEART.",
         "3.39",
         "Miscellaneous",
         "Flores LLC"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>UnitCost</th>\n",
       "      <th>Category</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>2.55</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Carpenter, Burton and Oneal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Francis-Mann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Lara-Baker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Diaz-Schaefer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Flores LLC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ProductID                          ProductName  UnitCost       Category  \\\n",
       "0    85123A   WHITE HANGING HEART T-LIGHT HOLDER      2.55  Miscellaneous   \n",
       "1     71053                  WHITE METAL LANTERN      3.39  Miscellaneous   \n",
       "2    84406B       CREAM CUPID HEARTS COAT HANGER      2.75  Miscellaneous   \n",
       "3    84029G  KNITTED UNION FLAG HOT WATER BOTTLE      3.39  Miscellaneous   \n",
       "4    84029E       RED WOOLLY HOTTIE WHITE HEART.      3.39  Miscellaneous   \n",
       "\n",
       "                         Brand  \n",
       "0  Carpenter, Burton and Oneal  \n",
       "1                 Francis-Mann  \n",
       "2                   Lara-Baker  \n",
       "3                Diaz-Schaefer  \n",
       "4                   Flores LLC  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# 4. Create Product Dimension Table (product_dim)\n",
    "# Extract unique products from the original dataset: StockCode, Description, UnitPrice.\n",
    "product_dim = df[['StockCode', 'Description', 'UnitPrice']].drop_duplicates().copy()\n",
    "\n",
    "# Rename columns to fit dimensional model schema\n",
    "product_dim = product_dim.rename(columns={\n",
    "    'StockCode': 'ProductID',\n",
    "    'Description': 'ProductName',\n",
    "    'UnitPrice': 'UnitCost'\n",
    "})\n",
    "\n",
    "# Define a simple function to categorize products based on keywords in ProductName\n",
    "def categorize_product(name):\n",
    "    if pd.isna(name):\n",
    "        return 'Miscellaneous'\n",
    "    name = name.lower()\n",
    "    if any(keyword in name for keyword in ['electronic', 'computer', 'usb', 'laptop', 'cable']):\n",
    "        return 'Electronics'\n",
    "    elif any(keyword in name for keyword in ['shirt', 'clothing', 'dress', 't-shirt', 'jeans']):\n",
    "        return 'Clothing'\n",
    "    elif any(keyword in name for keyword in ['book', 'novel', 'journal']):\n",
    "        return 'Books'\n",
    "    elif any(keyword in name for keyword in ['toy', 'game']):\n",
    "        return 'Toys & Games'\n",
    "    else:\n",
    "        return 'Miscellaneous'\n",
    "\n",
    "# Apply the category function to create a Category column\n",
    "product_dim['Category'] = product_dim['ProductName'].apply(categorize_product)\n",
    "\n",
    "# Generate a synthetic Brand name using Faker company names for each product\n",
    "product_dim['Brand'] = [fake.company() for _ in range(len(product_dim))]\n",
    "\n",
    "# Display sample of product_dim to verify\n",
    "product_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b13c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'InvoiceDate' to datetime if not already\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Create a new column 'InvoiceDateOnly' normalized to midnight (date only, no time)\n",
    "df['InvoiceDateOnly'] = df['InvoiceDate'].dt.normalize()\n",
    "\n",
    "# Ensure 'FullDate' in time_dim is datetime type for correct merging\n",
    "time_dim['FullDate'] = pd.to_datetime(time_dim['FullDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c621d",
   "metadata": {},
   "source": [
    "### *Prepare FactSales Table (Fact Table)*\n",
    "\n",
    "*Description:*\n",
    "\n",
    "*This final chunk assembles the FactSales table, which records individual sales transactions linked to the various dimension tables through foreign keys. First, any encoding issues in column names are corrected to ensure consistency. Duplicate columns created through multiple merges are removed to avoid confusion and errors. The fact table is enriched by merging the Time Dimension to include a `TimeID` foreign key, facilitating time-based joins and analysis. Invoice dates are normalized to exclude time information, aligning with the date-only nature of the Time Dimension. The product identifier is standardized by assigning `ProductID` as the original stock code. The store foreign key (`StoreID`) is merged in based on country, linking sales to store locations. A key metric, `TotalSales`, is calculated by multiplying the quantity sold by the unit price, providing the total revenue per transaction line. Finally, only the relevant columns necessary for the fact table schema are selected to form the `fact_sales` DataFrame, ready for analytical queries or database loading.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0028bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'ï»¿InvoiceNo': 'InvoiceNo'})\n",
    "# Use the 'TimeID' and 'StoreID' columns without suffixes if present\n",
    "if 'TimeID' not in df.columns:\n",
    "    if 'TimeID_y' in df.columns:\n",
    "        df['TimeID'] = df['TimeID_y']\n",
    "    elif 'TimeID_x' in df.columns:\n",
    "        df['TimeID'] = df['TimeID_x']\n",
    "\n",
    "if 'StoreID' not in df.columns:\n",
    "    if 'StoreID_y' in df.columns:\n",
    "        df['StoreID'] = df['StoreID_y']\n",
    "    elif 'StoreID_x' in df.columns:\n",
    "        df['StoreID'] = df['StoreID_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b283e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94932c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df with time_dim to get TimeID by matching on normalized date columns\n",
    "df = df.merge(\n",
    "    time_dim[['TimeID', 'FullDate']],\n",
    "    left_on='InvoiceDateOnly',\n",
    "    right_on='FullDate',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert 'InvoiceDate' column to just date (drop time component) for fact table compatibility\n",
    "df['InvoiceDate'] = df['InvoiceDate'].dt.date\n",
    "\n",
    "# Assign 'ProductID' as the same value as 'StockCode' for clarity and schema matching\n",
    "df['ProductID'] = df['StockCode']\n",
    "\n",
    "# Merge df with store_dim on 'Country' to get StoreID foreign key\n",
    "df = df.merge(\n",
    "    store_dim[['StoreID', 'Country']],\n",
    "    on='Country',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate total sales amount per transaction line\n",
    "df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "#  Add Discount column if missing (assumed 0)\n",
    "if 'Discount' not in df.columns:\n",
    "    df['Discount'] = 0\n",
    "\n",
    "# Then select the columns including Discount\n",
    "fact_sales = df[['InvoiceNo', 'InvoiceDate', 'TimeID', 'ProductID', 'CustomerID', 'StoreID',\n",
    "                 'Quantity', 'UnitPrice', 'Discount', 'TotalSales']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57400c",
   "metadata": {},
   "source": [
    "## *Extract , Transform & Load:Retail_Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ac9f1",
   "metadata": {},
   "source": [
    "### *Imports and Logging Setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3589a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9010a81",
   "metadata": {},
   "source": [
    "## *Step 1: Extraction Phase*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04e7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(csv_folder, tables):\n",
    "    \"\"\"\n",
    "    Extract CSV files into pandas DataFrames.\n",
    "    Converts InvoiceDate to datetime for FactSales.\n",
    "    Returns a dictionary of {table_name: DataFrame}.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "    logging.info(\"=== ETL PROCESS STARTED ===\")\n",
    "    logging.info(\"STEP 1: Extraction\")\n",
    "    \n",
    "    for table in tables:\n",
    "        logging.info(f\"Extracting data for table '{table}' from CSV...\")\n",
    "        file_path = os.path.join(csv_folder, f\"{table}.csv\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "            if 'InvoiceDate' in df.columns:\n",
    "                df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "                df = df.dropna(subset=['InvoiceDate'])\n",
    "            dataframes[table] = df\n",
    "            logging.info(f\"✔ Extracted {len(df)} rows from '{table}'\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting {table}: {e}\")\n",
    "            dataframes[table] = pd.DataFrame()\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4013943",
   "metadata": {},
   "source": [
    "## *Step_2:Transformation Phase*\n",
    "*Following the full extraction, we proceeded with a`* ***Full Transformation*** *approach. Each dataset was fully inspected and cleaned independently to ensure data quality and consistency before merging.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98b7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataframes):\n",
    "    \"\"\"\n",
    "    Transform DataFrames:\n",
    "    - FactSales: filter invalid rows, compute TotalSales, filter last year's sales\n",
    "    - CustomerDim, StoreDim, TimeDim: remove duplicates, handle ID columns\n",
    "    - ProductDim: remove duplicates, convert column types, remove invalid UnitPrice\n",
    "    \"\"\"\n",
    "    logging.info(\"STEP 2: Transformation\")\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Table '{table_name}' is empty, skipping transformation\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Transforming table '{table_name}'...\")\n",
    "\n",
    "        # FactSales transformations\n",
    "        if table_name == \"FactSales\":\n",
    "            if 'Quantity' in df.columns and 'UnitPrice' in df.columns:\n",
    "                df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "                df.loc[:, 'TotalSales'] = round(df['Quantity'] * df['UnitPrice'], 2)\n",
    "            if 'InvoiceDate' in df.columns:\n",
    "                cutoff = pd.Timestamp('2024-08-12')\n",
    "                df = df[df['InvoiceDate'] >= cutoff]\n",
    "                df['InvoiceDate'] = df['InvoiceDate'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # CustomerDim, StoreDim, TimeDim transformations\n",
    "        elif table_name in [\"CustomerDim\", \"StoreDim\", \"TimeDim\"]:\n",
    "            df = df.drop_duplicates()\n",
    "            id_cols = [col for col in df.columns if \"ID\" in col]\n",
    "            if id_cols:\n",
    "                df = df.dropna(subset=id_cols)\n",
    "                for col in id_cols:\n",
    "                    df[col] = df[col].astype(int, errors='ignore')\n",
    "\n",
    "        # ProductDim specific cleaning\n",
    "        elif table_name == \"ProductDim\":\n",
    "            df = df.drop_duplicates()\n",
    "            for col in df.columns:\n",
    "                if col.lower() in ['stockcode', 'productid']:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                elif col.lower() in ['unitprice', 'unitcost']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df = df.dropna(subset=['UnitPrice']) if 'UnitPrice' in df.columns else df\n",
    "\n",
    "        dataframes[table_name] = df\n",
    "        logging.info(f\"✔ Table '{table_name}' transformed. {len(df)} rows ready for loading\")\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f86a3e",
   "metadata": {},
   "source": [
    "## *Step 3:Load to SQLite*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc7d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_db(dataframes, schema_file, db_path, csv_folder=None):\n",
    "    \"\"\"\n",
    "    Load transformed DataFrames into SQLite database.\n",
    "    Optionally, save transformed CSVs to csv_folder.\n",
    "    \"\"\"\n",
    "    logging.info(\"STEP 3: Loading into database\")\n",
    "\n",
    "    # Remove existing database if it exists\n",
    "    if os.path.exists(db_path):\n",
    "        logging.info(f\"Existing database found. Removing {db_path}\")\n",
    "        os.remove(db_path)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Apply schema\n",
    "    with open(schema_file, 'r') as f:\n",
    "        schema_sql = f.read()\n",
    "    cursor.executescript(schema_sql)\n",
    "    conn.commit()\n",
    "    logging.info(\"✔ Database schema applied successfully\")\n",
    "\n",
    "    # Insert data\n",
    "    for table_name, df in dataframes.items():\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Table '{table_name}' is empty, skipping load\")\n",
    "            continue\n",
    "        placeholders = ', '.join(['?'] * len(df.columns))\n",
    "        data_tuples = list(df.itertuples(index=False, name=None))\n",
    "        if table_name != \"FactSales\":\n",
    "            query = f\"INSERT OR IGNORE INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "        else:\n",
    "            query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "        try:\n",
    "            cursor.executemany(query, data_tuples)\n",
    "            conn.commit()\n",
    "            logging.info(f\"✔ Loaded {len(df)} rows into '{table_name}'\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {table_name}: {e}\")\n",
    "\n",
    "    conn.close()\n",
    "    logging.info(\"=== ETL PROCESS COMPLETED SUCCESSFULLY ===\")\n",
    "    logging.info(f\"Database created at: {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd4632",
   "metadata": {},
   "source": [
    "### *Master ETL Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a067b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_retail(csv_folder, schema_file, db_path):\n",
    "    \"\"\"\n",
    "    Master ETL function that runs extraction, transformation, and loading sequentially.\n",
    "    \"\"\"\n",
    "    tables = [\"FactSales\", \"CustomerDim\", \"StoreDim\", \"ProductDim\", \"TimeDim\"]\n",
    "\n",
    "    # Step 1: Extract\n",
    "    dataframes = extract_csv(csv_folder, tables)\n",
    "\n",
    "    # Step 2: Transform\n",
    "    dataframes = transform_data(dataframes)\n",
    "\n",
    "    # Step 3: Load\n",
    "    load_to_db(dataframes, schema_file, db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7f5e7",
   "metadata": {},
   "source": [
    "### *Run the ETL process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db5b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 00:35:39,712 - INFO - === ETL PROCESS STARTED ===\n",
      "2025-08-15 00:35:39,717 - INFO - STEP 1: Extraction\n",
      "2025-08-15 00:35:39,717 - INFO - Extracting data for table 'FactSales' from CSV...\n",
      "2025-08-15 00:35:41,088 - INFO - ✔ Extracted 541909 rows from 'FactSales'\n",
      "2025-08-15 00:35:41,088 - INFO - Extracting data for table 'CustomerDim' from CSV...\n",
      "2025-08-15 00:35:41,144 - INFO - ✔ Extracted 4389 rows from 'CustomerDim'\n",
      "2025-08-15 00:35:41,146 - INFO - Extracting data for table 'StoreDim' from CSV...\n",
      "2025-08-15 00:35:41,150 - INFO - ✔ Extracted 38 rows from 'StoreDim'\n",
      "2025-08-15 00:35:41,158 - INFO - Extracting data for table 'ProductDim' from CSV...\n",
      "2025-08-15 00:35:41,245 - INFO - ✔ Extracted 18053 rows from 'ProductDim'\n",
      "2025-08-15 00:35:41,249 - INFO - Extracting data for table 'TimeDim' from CSV...\n",
      "2025-08-15 00:35:41,258 - INFO - ✔ Extracted 305 rows from 'TimeDim'\n",
      "2025-08-15 00:35:41,261 - INFO - STEP 2: Transformation\n",
      "2025-08-15 00:35:41,263 - INFO - Transforming table 'FactSales'...\n",
      "2025-08-15 00:35:42,792 - INFO - ✔ Table 'FactSales' transformed. 530104 rows ready for loading\n",
      "2025-08-15 00:35:42,809 - INFO - Transforming table 'CustomerDim'...\n",
      "2025-08-15 00:35:42,821 - INFO - ✔ Table 'CustomerDim' transformed. 4380 rows ready for loading\n",
      "2025-08-15 00:35:42,821 - INFO - Transforming table 'StoreDim'...\n",
      "2025-08-15 00:35:42,829 - INFO - ✔ Table 'StoreDim' transformed. 38 rows ready for loading\n",
      "2025-08-15 00:35:42,829 - INFO - Transforming table 'ProductDim'...\n",
      "2025-08-15 00:35:42,856 - INFO - ✔ Table 'ProductDim' transformed. 18053 rows ready for loading\n",
      "2025-08-15 00:35:42,859 - INFO - Transforming table 'TimeDim'...\n",
      "2025-08-15 00:35:42,871 - INFO - ✔ Table 'TimeDim' transformed. 305 rows ready for loading\n",
      "2025-08-15 00:35:42,872 - INFO - STEP 3: Loading into database\n",
      "2025-08-15 00:35:42,872 - INFO - Existing database found. Removing C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\retail_dw.db\n",
      "2025-08-15 00:35:42,930 - INFO - ✔ Database schema applied successfully\n",
      "2025-08-15 00:35:45,194 - INFO - ✔ Loaded 530104 rows into 'FactSales'\n",
      "2025-08-15 00:35:45,277 - INFO - ✔ Loaded 4380 rows into 'CustomerDim'\n",
      "2025-08-15 00:35:45,277 - INFO - ✔ Loaded 38 rows into 'StoreDim'\n",
      "2025-08-15 00:35:45,339 - INFO - ✔ Loaded 18053 rows into 'ProductDim'\n",
      "2025-08-15 00:35:45,344 - INFO - ✔ Loaded 305 rows into 'TimeDim'\n",
      "2025-08-15 00:35:45,344 - INFO - === ETL PROCESS COMPLETED SUCCESSFULLY ===\n",
      "2025-08-15 00:35:45,344 - INFO - Database created at: C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\retail_dw.db\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "csv_folder = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\"\n",
    "schema_file = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_1_Data_Warehouse_Design\\schema_design.sql\"\n",
    "db_path = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\retail_dw.db\"\n",
    "\n",
    "# Run ETL\n",
    "etl_retail(csv_folder, schema_file, db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c9092",
   "metadata": {},
   "source": [
    "### *Checking the loading the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9279502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in DB: [('TimeDim',), ('CustomerDim',), ('ProductDim',), ('StoreDim',), ('FactSales',), ('sqlite_sequence',)]\n",
      "(1, '536365', '2024-12-01 00:00:00', 20241201, '85123A', 17850, 1, 6, 2.55, 0.0, 15.3)\n",
      "(2, '536365', '2024-12-01 00:00:00', 20241201, 71053, 17850, 1, 6, 3.39, 0.0, 20.34)\n",
      "(3, '536365', '2024-12-01 00:00:00', 20241201, '84406B', 17850, 1, 8, 2.75, 0.0, 22.0)\n",
      "(4, '536365', '2024-12-01 00:00:00', 20241201, '84029G', 17850, 1, 6, 3.39, 0.0, 20.34)\n",
      "(5, '536365', '2024-12-01 00:00:00', 20241201, '84029E', 17850, 1, 6, 3.39, 0.0, 20.34)\n",
      "(6, '536365', '2024-12-01 00:00:00', 20241201, 22752, 17850, 1, 2, 7.65, 0.0, 15.3)\n",
      "(7, '536365', '2024-12-01 00:00:00', 20241201, 21730, 17850, 1, 6, 4.25, 0.0, 25.5)\n",
      "(8, '536366', '2024-12-01 00:00:00', 20241201, 22633, 17850, 1, 6, 1.85, 0.0, 11.1)\n",
      "(9, '536366', '2024-12-01 00:00:00', 20241201, 22632, 17850, 1, 6, 1.85, 0.0, 11.1)\n",
      "(10, '536367', '2024-12-01 00:00:00', 20241201, 84879, 13047, 1, 32, 1.69, 0.0, 54.08)\n",
      "(11, '536367', '2024-12-01 00:00:00', 20241201, 22745, 13047, 1, 6, 2.1, 0.0, 12.6)\n",
      "(12, '536367', '2024-12-01 00:00:00', 20241201, 22748, 13047, 1, 6, 2.1, 0.0, 12.6)\n",
      "(13, '536367', '2024-12-01 00:00:00', 20241201, 22749, 13047, 1, 8, 3.75, 0.0, 30.0)\n",
      "(14, '536367', '2024-12-01 00:00:00', 20241201, 22310, 13047, 1, 6, 1.65, 0.0, 9.9)\n",
      "(15, '536367', '2024-12-01 00:00:00', 20241201, 84969, 13047, 1, 6, 4.25, 0.0, 25.5)\n",
      "(16, '536367', '2024-12-01 00:00:00', 20241201, 22623, 13047, 1, 3, 4.95, 0.0, 14.85)\n",
      "(17, '536367', '2024-12-01 00:00:00', 20241201, 22622, 13047, 1, 2, 9.95, 0.0, 19.9)\n",
      "(18, '536367', '2024-12-01 00:00:00', 20241201, 21754, 13047, 1, 3, 5.95, 0.0, 17.85)\n",
      "(19, '536367', '2024-12-01 00:00:00', 20241201, 21755, 13047, 1, 3, 5.95, 0.0, 17.85)\n",
      "(20, '536367', '2024-12-01 00:00:00', 20241201, 21777, 13047, 1, 4, 7.95, 0.0, 31.8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_path = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\retail_dw.db\"\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# List all tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in DB:\", tables)\n",
    "\n",
    "# Query more rows (e.g., 20 rows)\n",
    "cursor.execute(\"SELECT * FROM FactSales LIMIT 20;\")\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".end-sem (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
