{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1db178",
   "metadata": {},
   "source": [
    "### *Load and preprocess the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e452857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import hashlib\n",
    "from faker import Faker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb90d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ca9aa",
   "metadata": {},
   "source": [
    "# *Data Generation: Dimensions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748c929",
   "metadata": {},
   "source": [
    "#### *Generate synthetic Customer Dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8917814",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "Faker.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffea8cd",
   "metadata": {},
   "source": [
    "*Description:*\n",
    "\n",
    "*This chunk focuses on loading the raw dataset from a CSV file into a pandas DataFrame, ensuring that special characters in the data are correctly handled by specifying the appropriate encoding. The 'InvoiceDate' column is explicitly converted into a datetime object, which enables more efficient and accurate manipulation of date and time data. To make the dataset appear current for analysis purposes, all invoice dates are shifted forward by 14 years, adjusting the original 2010-2011 timestamps to approximately 2024-2025. This simulated recency of the data can be important for testing or reporting. Finally, the new date range is printed as a sanity check to confirm that the shift was applied correctly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e508ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range after shifting: 2024-12-01 08:26:00 to 2025-12-09 12:50:00\n"
     ]
    }
   ],
   "source": [
    "# Load dataset CSV into pandas DataFrame.\n",
    "# Encoding ISO-8859-1 is used to handle special characters.\n",
    "df = pd.read_csv('../Data/Online_Retail.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Convert 'InvoiceDate' column to datetime type for easier date/time operations.\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Shift invoice dates forward by 14 years to simulate current data (2024-2025).\n",
    "df['InvoiceDate'] = df['InvoiceDate'] + pd.DateOffset(years=14)\n",
    "\n",
    "# Verify date range after shifting.\n",
    "print(\"Date range after shifting:\", df['InvoiceDate'].min(), \"to\", df['InvoiceDate'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb481e",
   "metadata": {},
   "source": [
    "### *Create Time Dimension Table (TimeDim)*\n",
    "*Description:*\n",
    "\n",
    "*In this step, a Time Dimension table is constructed, which is fundamental in data warehousing and analytical processing for providing rich temporal context to sales data. The process starts by extracting all unique dates from the transactional data and normalizing them to remove time components, ensuring that each date appears only once. A unique `TimeID` is generated for each date using the YYYYMMDD format, facilitating efficient joins with fact tables. Additional columns are created to break down each date into components such as day, month, quarter, year, and week number — all valuable for time-based grouping and trend analysis. This denormalized structure simplifies querying and reporting over time.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155e0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Time Dimension (TimeDim) table\n",
    "# Create empty DataFrame for Time Dimension\n",
    "time_dim = pd.DataFrame()\n",
    "\n",
    "# Extract unique dates from the 'InvoiceDate' column (date only, no time)\n",
    "time_dim['FullDate'] = pd.to_datetime(df['InvoiceDate'].dt.date.unique())\n",
    "\n",
    "# Generate a unique TimeID for each date in YYYYMMDD integer format\n",
    "time_dim['TimeID'] = time_dim['FullDate'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# Extract useful date attributes for analysis\n",
    "time_dim['Day'] = time_dim['FullDate'].dt.day\n",
    "time_dim['Month'] = time_dim['FullDate'].dt.month\n",
    "time_dim['Quarter'] = time_dim['FullDate'].dt.quarter\n",
    "time_dim['Year'] = time_dim['FullDate'].dt.year\n",
    "time_dim['WeekOfYear'] = time_dim['FullDate'].dt.isocalendar().week\n",
    "\n",
    "# Reorder columns for clarity\n",
    "time_dim = time_dim[['TimeID', 'FullDate', 'Day', 'Month', 'Quarter', 'Year', 'WeekOfYear']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a56ff",
   "metadata": {},
   "source": [
    "### *Create Customer Dimension Table (CustomerDim)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e72f7b",
   "metadata": {},
   "source": [
    "*Description:*\n",
    "\n",
    "*This chunk builds the Customer Dimension table, which profiles unique customers using a combination of actual and synthetic data. The real `CustomerID` and country information are directly extracted to maintain referential integrity. Since personal identifying information like names and cities are not available or desirable to use, synthetic values are generated to enrich the dataset while preserving privacy. Customer names are created by hashing the `CustomerID` to produce consistent yet anonymous identifiers. Cities are generated using the Faker library with locale settings based on the customer’s country, adding realistic geographic diversity. Additionally, plausible gender and age values are randomly assigned within reasonable bounds to simulate demographic attributes. Finally, the earliest invoice date is used as a proxy for the customer’s registration date, providing a temporal reference for customer activity.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2808f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CustomerID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CustomerName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "City",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CustomerSince",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "6e0c7ebe-dc29-4073-89b1-ce07d7092953",
       "rows": [
        [
         "0",
         "17850.0",
         "United Kingdom",
         "54cde5dbb6",
         "North Henrybury",
         "Other",
         "71",
         "2024-12-01 08:26:00"
        ],
        [
         "9",
         "13047.0",
         "United Kingdom",
         "86314fa849",
         "East Timothy",
         "Male",
         "69",
         "2024-12-01 08:26:00"
        ],
        [
         "26",
         "12583.0",
         "France",
         "dcff63cd99",
         "New Roberttown",
         "Male",
         "74",
         "2024-12-01 08:26:00"
        ],
        [
         "46",
         "13748.0",
         "United Kingdom",
         "590354e49f",
         "East Donaldhaven",
         "Other",
         "49",
         "2024-12-01 08:26:00"
        ],
        [
         "65",
         "15100.0",
         "United Kingdom",
         "58ec7997a6",
         "New Joeside",
         "Female",
         "52",
         "2024-12-01 08:26:00"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "      <th>CustomerName</th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>CustomerSince</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>54cde5dbb6</td>\n",
       "      <td>North Henrybury</td>\n",
       "      <td>Other</td>\n",
       "      <td>71</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13047.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>86314fa849</td>\n",
       "      <td>East Timothy</td>\n",
       "      <td>Male</td>\n",
       "      <td>69</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12583.0</td>\n",
       "      <td>France</td>\n",
       "      <td>dcff63cd99</td>\n",
       "      <td>New Roberttown</td>\n",
       "      <td>Male</td>\n",
       "      <td>74</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>13748.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>590354e49f</td>\n",
       "      <td>East Donaldhaven</td>\n",
       "      <td>Other</td>\n",
       "      <td>49</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>15100.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>58ec7997a6</td>\n",
       "      <td>New Joeside</td>\n",
       "      <td>Female</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-12-01 08:26:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CustomerID         Country CustomerName              City  Gender  Age  \\\n",
       "0      17850.0  United Kingdom   54cde5dbb6   North Henrybury   Other   71   \n",
       "9      13047.0  United Kingdom   86314fa849      East Timothy    Male   69   \n",
       "26     12583.0          France   dcff63cd99    New Roberttown    Male   74   \n",
       "46     13748.0  United Kingdom   590354e49f  East Donaldhaven   Other   49   \n",
       "65     15100.0  United Kingdom   58ec7997a6       New Joeside  Female   52   \n",
       "\n",
       "         CustomerSince  \n",
       "0  2024-12-01 08:26:00  \n",
       "9  2024-12-01 08:26:00  \n",
       "26 2024-12-01 08:26:00  \n",
       "46 2024-12-01 08:26:00  \n",
       "65 2024-12-01 08:26:00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "def hash_customer_name(cust_id):\n",
    "    # Generate a synthetic name by hashing the CustomerID\n",
    "    return hashlib.sha256(str(cust_id).encode()).hexdigest()[:10]\n",
    "\n",
    "def generate_city_based_on_country(country):\n",
    "    # Use Faker locale based on country for city name if possible, else default locale\n",
    "    # Here we simplify: if country is UK use en_GB, else en_US or default\n",
    "    if country == 'United Kingdom':\n",
    "        fake_local = Faker('en_GB')\n",
    "    else:\n",
    "        fake_local = Faker()\n",
    "    return fake_local.city()\n",
    "\n",
    "# Extract unique customers with country\n",
    "customer_dim = df[['CustomerID', 'Country']].drop_duplicates().copy()\n",
    "\n",
    "# Create synthetic CustomerName by hashing CustomerID\n",
    "customer_dim['CustomerName'] = customer_dim['CustomerID'].apply(hash_customer_name)\n",
    "\n",
    "# Generate synthetic City based on Country\n",
    "customer_dim['City'] = customer_dim['Country'].apply(generate_city_based_on_country)\n",
    "\n",
    "# Generate reasonable synthetic Gender and Age\n",
    "gender_choices = ['Male', 'Female', 'Other']\n",
    "customer_dim['Gender'] = [random.choice(gender_choices) for _ in range(len(customer_dim))]\n",
    "customer_dim['Age'] = [random.randint(18, 75) for _ in range(len(customer_dim))]\n",
    "\n",
    "# Set CustomerSince as earliest InvoiceDate in the dataset\n",
    "customer_dim['CustomerSince'] = df['InvoiceDate'].min()\n",
    "\n",
    "# Drop Email column (not required)\n",
    "# No Email column added here\n",
    "\n",
    "customer_dim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c2de9",
   "metadata": {},
   "source": [
    "### *Create Store Dimension Table (StoreDim)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c637d7",
   "metadata": {},
   "source": [
    "\n",
    "*Description:*\n",
    "\n",
    "*This step constructs the Store Dimension table, which represents the stores or sales locations for transactions. Since the dataset primarily references countries rather than specific store locations, each unique country is treated as a distinct store. Unique numeric `StoreID`s are assigned for efficient foreign key references. Store names are generated to include the country name for clarity and uniqueness. The sales channel is hardcoded as \"Online,\" reflecting the dataset's nature as online retail transactions. To provide richer location information, synthetic cities are generated for each store based on the country, using locale-specific Faker instances to maintain geographic plausibility. This approach allows analysis at the store level while enhancing location details without requiring real-world addresses.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c0afb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "StoreID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "StoreName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Channel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "City",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a70462fd-18dc-43c5-aeb5-acdca10fb42b",
       "rows": [
        [
         "0",
         "United Kingdom",
         "1",
         "Online Store - United Kingdom",
         "Online",
         "Kimberleychester"
        ],
        [
         "1",
         "France",
         "2",
         "Online Store - France",
         "Online",
         "Higginston"
        ],
        [
         "2",
         "Australia",
         "3",
         "Online Store - Australia",
         "Online",
         "Deborahmouth"
        ],
        [
         "3",
         "Netherlands",
         "4",
         "Online Store - Netherlands",
         "Online",
         "Berryhaven"
        ],
        [
         "4",
         "Germany",
         "5",
         "Online Store - Germany",
         "Online",
         "South Lisachester"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>StoreID</th>\n",
       "      <th>StoreName</th>\n",
       "      <th>Channel</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1</td>\n",
       "      <td>Online Store - United Kingdom</td>\n",
       "      <td>Online</td>\n",
       "      <td>Kimberleychester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>2</td>\n",
       "      <td>Online Store - France</td>\n",
       "      <td>Online</td>\n",
       "      <td>Higginston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>3</td>\n",
       "      <td>Online Store - Australia</td>\n",
       "      <td>Online</td>\n",
       "      <td>Deborahmouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>4</td>\n",
       "      <td>Online Store - Netherlands</td>\n",
       "      <td>Online</td>\n",
       "      <td>Berryhaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>5</td>\n",
       "      <td>Online Store - Germany</td>\n",
       "      <td>Online</td>\n",
       "      <td>South Lisachester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country  StoreID                      StoreName Channel  \\\n",
       "0  United Kingdom        1  Online Store - United Kingdom  Online   \n",
       "1          France        2          Online Store - France  Online   \n",
       "2       Australia        3       Online Store - Australia  Online   \n",
       "3     Netherlands        4     Online Store - Netherlands  Online   \n",
       "4         Germany        5         Online Store - Germany  Online   \n",
       "\n",
       "                City  \n",
       "0   Kimberleychester  \n",
       "1         Higginston  \n",
       "2       Deborahmouth  \n",
       "3         Berryhaven  \n",
       "4  South Lisachester  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique countries as stores\n",
    "store_dim = df[['Country']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Assign unique StoreID starting from 1\n",
    "store_dim['StoreID'] = store_dim.index + 1\n",
    "\n",
    "# Assign StoreName with country suffix for uniqueness\n",
    "store_dim['StoreName'] = store_dim['Country'].apply(lambda x: f\"Online Store - {x}\")\n",
    "\n",
    "# Assign Channel as 'Online' (dataset is online retail)\n",
    "store_dim['Channel'] = 'Online'\n",
    "\n",
    "# Generate synthetic City based on country using Faker locales\n",
    "def generate_city(country):\n",
    "    if country == 'United Kingdom':\n",
    "        fake_local = Faker('en_GB')\n",
    "    else:\n",
    "        fake_local = Faker()\n",
    "    return fake_local.city()\n",
    "\n",
    "store_dim['City'] = store_dim['Country'].apply(generate_city)\n",
    "\n",
    "store_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aade59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ProductID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ProductName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "UnitCost",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Brand",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "aa2e6fc1-82a8-4ce2-a188-2c72212ed53b",
       "rows": [
        [
         "0",
         "85123A",
         "WHITE HANGING HEART T-LIGHT HOLDER",
         "2.55",
         "Miscellaneous",
         "Carpenter, Burton and Oneal"
        ],
        [
         "1",
         "71053",
         "WHITE METAL LANTERN",
         "3.39",
         "Miscellaneous",
         "Francis-Mann"
        ],
        [
         "2",
         "84406B",
         "CREAM CUPID HEARTS COAT HANGER",
         "2.75",
         "Miscellaneous",
         "Lara-Baker"
        ],
        [
         "3",
         "84029G",
         "KNITTED UNION FLAG HOT WATER BOTTLE",
         "3.39",
         "Miscellaneous",
         "Diaz-Schaefer"
        ],
        [
         "4",
         "84029E",
         "RED WOOLLY HOTTIE WHITE HEART.",
         "3.39",
         "Miscellaneous",
         "Flores LLC"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>UnitCost</th>\n",
       "      <th>Category</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>2.55</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Carpenter, Burton and Oneal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Francis-Mann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Lara-Baker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Diaz-Schaefer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Flores LLC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ProductID                          ProductName  UnitCost       Category  \\\n",
       "0    85123A   WHITE HANGING HEART T-LIGHT HOLDER      2.55  Miscellaneous   \n",
       "1     71053                  WHITE METAL LANTERN      3.39  Miscellaneous   \n",
       "2    84406B       CREAM CUPID HEARTS COAT HANGER      2.75  Miscellaneous   \n",
       "3    84029G  KNITTED UNION FLAG HOT WATER BOTTLE      3.39  Miscellaneous   \n",
       "4    84029E       RED WOOLLY HOTTIE WHITE HEART.      3.39  Miscellaneous   \n",
       "\n",
       "                         Brand  \n",
       "0  Carpenter, Burton and Oneal  \n",
       "1                 Francis-Mann  \n",
       "2                   Lara-Baker  \n",
       "3                Diaz-Schaefer  \n",
       "4                   Flores LLC  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# 4. Create Product Dimension Table (product_dim)\n",
    "# ------------------------------------------------\n",
    "# Extract unique products from the original dataset: StockCode, Description, UnitPrice.\n",
    "product_dim = df[['StockCode', 'Description', 'UnitPrice']].drop_duplicates().copy()\n",
    "\n",
    "# Rename columns to fit dimensional model schema\n",
    "product_dim = product_dim.rename(columns={\n",
    "    'StockCode': 'ProductID',\n",
    "    'Description': 'ProductName',\n",
    "    'UnitPrice': 'UnitCost'\n",
    "})\n",
    "\n",
    "# Define a simple function to categorize products based on keywords in ProductName\n",
    "def categorize_product(name):\n",
    "    if pd.isna(name):\n",
    "        return 'Miscellaneous'\n",
    "    name = name.lower()\n",
    "    if any(keyword in name for keyword in ['electronic', 'computer', 'usb', 'laptop', 'cable']):\n",
    "        return 'Electronics'\n",
    "    elif any(keyword in name for keyword in ['shirt', 'clothing', 'dress', 't-shirt', 'jeans']):\n",
    "        return 'Clothing'\n",
    "    elif any(keyword in name for keyword in ['book', 'novel', 'journal']):\n",
    "        return 'Books'\n",
    "    elif any(keyword in name for keyword in ['toy', 'game']):\n",
    "        return 'Toys & Games'\n",
    "    else:\n",
    "        return 'Miscellaneous'\n",
    "\n",
    "# Apply the category function to create a Category column\n",
    "product_dim['Category'] = product_dim['ProductName'].apply(categorize_product)\n",
    "\n",
    "# Generate a synthetic Brand name using Faker company names for each product\n",
    "product_dim['Brand'] = [fake.company() for _ in range(len(product_dim))]\n",
    "\n",
    "# Display sample of product_dim to verify\n",
    "product_dim.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b13c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'InvoiceDate' to datetime if not already\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Create a new column 'InvoiceDateOnly' normalized to midnight (date only, no time)\n",
    "df['InvoiceDateOnly'] = df['InvoiceDate'].dt.normalize()\n",
    "\n",
    "# Ensure 'FullDate' in time_dim is datetime type for correct merging\n",
    "time_dim['FullDate'] = pd.to_datetime(time_dim['FullDate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c621d",
   "metadata": {},
   "source": [
    "### *Prepare FactSales Table (Fact Table)*\n",
    "\n",
    "*Description:*\n",
    "\n",
    "*This final chunk assembles the FactSales table, which records individual sales transactions linked to the various dimension tables through foreign keys. First, any encoding issues in column names are corrected to ensure consistency. Duplicate columns created through multiple merges are removed to avoid confusion and errors. The fact table is enriched by merging the Time Dimension to include a `TimeID` foreign key, facilitating time-based joins and analysis. Invoice dates are normalized to exclude time information, aligning with the date-only nature of the Time Dimension. The product identifier is standardized by assigning `ProductID` as the original stock code. The store foreign key (`StoreID`) is merged in based on country, linking sales to store locations. A key metric, `TotalSales`, is calculated by multiplying the quantity sold by the unit price, providing the total revenue per transaction line. Finally, only the relevant columns necessary for the fact table schema are selected to form the `fact_sales` DataFrame, ready for analytical queries or database loading.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0028bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'ï»¿InvoiceNo': 'InvoiceNo'})\n",
    "# Use the 'TimeID' and 'StoreID' columns without suffixes if present\n",
    "if 'TimeID' not in df.columns:\n",
    "    if 'TimeID_y' in df.columns:\n",
    "        df['TimeID'] = df['TimeID_y']\n",
    "    elif 'TimeID_x' in df.columns:\n",
    "        df['TimeID'] = df['TimeID_x']\n",
    "\n",
    "if 'StoreID' not in df.columns:\n",
    "    if 'StoreID_y' in df.columns:\n",
    "        df['StoreID'] = df['StoreID_y']\n",
    "    elif 'StoreID_x' in df.columns:\n",
    "        df['StoreID'] = df['StoreID_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b283e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94932c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df with time_dim to get TimeID by matching on normalized date columns\n",
    "df = df.merge(\n",
    "    time_dim[['TimeID', 'FullDate']],\n",
    "    left_on='InvoiceDateOnly',\n",
    "    right_on='FullDate',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert 'InvoiceDate' column to just date (drop time component) for fact table compatibility\n",
    "df['InvoiceDate'] = df['InvoiceDate'].dt.date\n",
    "\n",
    "# Assign 'ProductID' as the same value as 'StockCode' for clarity and schema matching\n",
    "df['ProductID'] = df['StockCode']\n",
    "\n",
    "# Merge df with store_dim on 'Country' to get StoreID foreign key\n",
    "df = df.merge(\n",
    "    store_dim[['StoreID', 'Country']],\n",
    "    on='Country',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate total sales amount per transaction line\n",
    "df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "#  Add Discount column if missing (assumed 0)\n",
    "if 'Discount' not in df.columns:\n",
    "    df['Discount'] = 0\n",
    "\n",
    "# Then select the columns including Discount\n",
    "fact_sales = df[['InvoiceNo', 'InvoiceDate', 'TimeID', 'ProductID', 'CustomerID', 'StoreID',\n",
    "                 'Quantity', 'UnitPrice', 'Discount', 'TotalSales']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c31663de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files saved in: C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1 Task_2_ETL_Process_Implementation\\Synthetic_data\n",
      "Files in folder now: ['CustomerDim.csv', 'FactSales.csv', 'ProductDim.csv', 'StoreDim.csv', 'TimeDim.csv']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Define main folder path ---\n",
    "folder_path = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1 Task_2_ETL_Process_Implementation\"\n",
    "\n",
    "# --- Step 2: Define Synthetic_data subfolder ---\n",
    "synthetic_folder = os.path.join(folder_path, 'Synthetic_data')\n",
    "os.makedirs(synthetic_folder, exist_ok=True)\n",
    "\n",
    "# --- Step 3: Save CSVs in Synthetic_data folder ---\n",
    "time_dim.to_csv(os.path.join(synthetic_folder, 'TimeDim.csv'), index=False)\n",
    "customer_dim.to_csv(os.path.join(synthetic_folder, 'CustomerDim.csv'), index=False)\n",
    "store_dim.to_csv(os.path.join(synthetic_folder, 'StoreDim.csv'), index=False)\n",
    "product_dim.to_csv(os.path.join(synthetic_folder, 'ProductDim.csv'), index=False)\n",
    "fact_sales.to_csv(os.path.join(synthetic_folder, 'FactSales.csv'), index=False)\n",
    "\n",
    "print(f\"All CSV files saved in: {synthetic_folder}\")\n",
    "print(\"Files in folder now:\", os.listdir(synthetic_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57400c",
   "metadata": {},
   "source": [
    "## *Extract , Transform & Load:Retail_Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97287be3",
   "metadata": {},
   "source": [
    "### *Step 1: Extract*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d777510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where CSVs are stored\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "folder_path = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1 Task_2_ETL_Process_Implementation\"\n",
    "data_folder = os.path.join(folder_path, \"Synthetic_data\") if os.path.isdir(os.path.join(folder_path, \"Synthetic_data\")) else folder_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c112c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 12:37:24,531 - INFO - Starting data extraction for C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\FactSales.csv\n",
      "2025-08-13 12:37:25,485 - INFO - Extracted 541909 rows from C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\FactSales.csv\n",
      "2025-08-13 12:37:25,516 - INFO - Starting data extraction for C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\CustomerDim.csv\n",
      "2025-08-13 12:37:25,537 - INFO - Extracted 4389 rows from C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\CustomerDim.csv\n",
      "2025-08-13 12:37:25,539 - INFO - Starting data extraction for C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\StoreDim.csv\n",
      "2025-08-13 12:37:25,544 - INFO - Extracted 38 rows from C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\StoreDim.csv\n",
      "2025-08-13 12:37:25,545 - INFO - Starting data extraction for C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\ProductDim.csv\n",
      "2025-08-13 12:37:25,590 - INFO - Extracted 18053 rows from C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\ProductDim.csv\n",
      "2025-08-13 12:37:25,592 - INFO - Starting data extraction for C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\TimeDim.csv\n",
      "2025-08-13 12:37:25,596 - INFO - Extracted 305 rows from C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\\TimeDim.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactSales: 541909 rows\n",
      "CustomerDim: 4389 rows\n",
      "StoreDim: 38 rows\n",
      "ProductDim: 18053 rows\n",
      "TimeDim: 305 rows\n"
     ]
    }
   ],
   "source": [
    "# List of CSV files\n",
    "tables = [\"FactSales\", \"CustomerDim\", \"StoreDim\", \"ProductDim\", \"TimeDim\"]\n",
    "\n",
    "# Folder containing the synthetic CSV files\n",
    "folder_path = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\synthetic_data\"\n",
    "def extract(file_path):\n",
    "    \"\"\"Extract data from a CSV file into a DataFrame.\"\"\"\n",
    "    logging.info(f\"Starting data extraction for {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "        # Only FactSales has InvoiceDate, convert if present\n",
    "        if 'InvoiceDate' in df.columns:\n",
    "            df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "            df = df.dropna(subset=['InvoiceDate'])\n",
    "        logging.info(f\"Extracted {len(df)} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during extraction: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "\n",
    "# Dictionary to store all extracted DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "for table in tables:\n",
    "    file_path = os.path.join(folder_path, f\"{table}.csv\")\n",
    "    df = extract(file_path)\n",
    "    dataframes[table] = df\n",
    "\n",
    "# Quick check\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd969132",
   "metadata": {},
   "source": [
    "# *T-Transform*\n",
    "### *Transformation Strategy*\n",
    "\n",
    "*Following the full extraction, we proceeded with a`* ***Full Transformation*** *approach. Each dataset was fully inspected and cleaned independently to ensure data quality and consistency before merging.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155cd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    logging.info(\"Starting transformation\")\n",
    "    # Filter out invalid sales: Quantity < 0 or UnitPrice <= 0\n",
    "    df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "\n",
    "    # Calculate TotalSales\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "    # Filter for sales in the last year (relative to 2025-08-12)\n",
    "    cutoff_date = pd.Timestamp('2024-08-12')\n",
    "    df = df[df['InvoiceDate'] >= cutoff_date]\n",
    "\n",
    "    logging.info(f\"Transformed data has {len(df)} rows after filtering\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13de8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 12:37:25,629 - INFO - Starting transformation\n",
      "C:\\Users\\Snit Kahsay\\AppData\\Local\\Temp\\ipykernel_26152\\2525994712.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
      "2025-08-13 12:37:25,849 - INFO - Transformed data has 530104 rows after filtering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InvoiceNo InvoiceDate    TimeID ProductID  CustomerID  StoreID  Quantity  \\\n",
      "0    536365  2024-12-01  20241201    85123A     17850.0        1         6   \n",
      "1    536365  2024-12-01  20241201     71053     17850.0        1         6   \n",
      "2    536365  2024-12-01  20241201    84406B     17850.0        1         8   \n",
      "3    536365  2024-12-01  20241201    84029G     17850.0        1         6   \n",
      "4    536365  2024-12-01  20241201    84029E     17850.0        1         6   \n",
      "\n",
      "   UnitPrice  Discount  TotalSales  \n",
      "0       2.55         0       15.30  \n",
      "1       3.39         0       20.34  \n",
      "2       2.75         0       22.00  \n",
      "3       3.39         0       20.34  \n",
      "4       3.39         0       20.34  \n",
      "Total rows after transformation: 530104\n"
     ]
    }
   ],
   "source": [
    "# Transform the extracted FactSales DataFrame\n",
    "fact_sales_transformed = transform(dataframes[\"FactSales\"])\n",
    "\n",
    "# Check result\n",
    "print(fact_sales_transformed.head())\n",
    "print(f\"Total rows after transformation: {len(fact_sales_transformed)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84a454",
   "metadata": {},
   "source": [
    "# *Load to SQLite*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b10218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 12:37:25,910 - INFO - Connected to database: C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\retail_dw.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 12:37:29,310 - INFO - Loaded 530104 rows into FactSales\n",
      "2025-08-13 12:37:29,353 - INFO - Loaded 4389 rows into CustomerDim\n",
      "2025-08-13 12:37:29,383 - INFO - Loaded 38 rows into StoreDim\n",
      "2025-08-13 12:37:29,475 - INFO - Loaded 18053 rows into ProductDim\n",
      "2025-08-13 12:37:29,498 - INFO - Loaded 305 rows into TimeDim\n",
      "2025-08-13 12:37:29,500 - INFO - All tables loaded and committed successfully.\n",
      "2025-08-13 12:37:29,504 - INFO - Database connection closed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Snit Kahsay\\\\Desktop\\\\DSA-2040_Practical_Exam_SnitTeshome552\\\\Section_1\\\\Task_2_ETL_Process_Implementation\\\\retail_dw.db'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def load_to_db(dataframes, db_path='retail_dw.db'):\n",
    "    \"\"\"\n",
    "    Load DataFrames into SQLite database.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes (dict): Dictionary of DataFrames to load {table_name: df}\n",
    "        db_path (str): Full path to SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        logging.info(f\"Connected to database: {db_path}\")\n",
    "\n",
    "        for table_name, df in dataframes.items():\n",
    "            if df.empty:\n",
    "                logging.warning(f\"{table_name} is empty, skipping.\")\n",
    "                continue\n",
    "\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            logging.info(f\"Loaded {len(df)} rows into {table_name}\")\n",
    "\n",
    "        conn.commit()\n",
    "        logging.info(\"All tables loaded and committed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data into database: {e}\")\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        logging.info(\"Database connection closed.\")\n",
    "\n",
    "    return db_path\n",
    "\n",
    "# Example usage\n",
    "# Make sure to include transformed FactSales\n",
    "dataframes_to_load = dataframes.copy()\n",
    "dataframes_to_load['FactSales'] = fact_sales_transformed  # Replace with transformed\n",
    "\n",
    "db_path = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_2_ETL_Process_Implementation\\retail_dw.db\"\n",
    "\n",
    "load_to_db(dataframes_to_load, db_path=db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd58dbe",
   "metadata": {},
   "source": [
    "#### *Loaing using Parquet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10aa806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source folder where Task 2 Parquet files are saved\n",
    "task2_folder = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1 Task_2_ETL_Process_Implementation\"\n",
    "\n",
    "# Single Loaded_data folder (from Task 1) to store all Parquet files\n",
    "loaded_data_folder = r\"C:\\Users\\Snit Kahsay\\Desktop\\DSA-2040_Practical_Exam_SnitTeshome552\\Section_1\\Task_1_Data_Warehouse_Design\\Loaded_data\"\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(loaded_data_folder, exist_ok=True)\n",
    "\n",
    "# Copy Task 2 Parquet files into the single Loaded_data folder\n",
    "for file in os.listdir(task2_folder):\n",
    "    if file.endswith('.parquet'):\n",
    "        src_path = os.path.join(task2_folder, file)\n",
    "        dst_path = os.path.join(loaded_data_folder, file)\n",
    "        # Avoid overwriting existing files\n",
    "        if not os.path.exists(dst_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"{file} already exists in Loaded_data folder, skipping copy.\")\n",
    "\n",
    "# Load all Parquet files from the single Loaded_data folder\n",
    "dataframes = {}\n",
    "for file in os.listdir(loaded_data_folder):\n",
    "    if file.endswith('.parquet'):\n",
    "        parquet_path = os.path.join(loaded_data_folder, file)\n",
    "        df_name = file.replace('.parquet', '')\n",
    "        try:\n",
    "            df = pd.read_parquet(parquet_path)\n",
    "            dataframes[df_name] = df\n",
    "            print(f\"Loaded {file} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".end-sem (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
